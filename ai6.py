"""
Fixed Arabic Voice Assistant - Streamlit Interface
With multiple speech recognition options
"""

import streamlit as st
import sounddevice as sd
import numpy as np
import wave
import tempfile
import os
import base64
from pathlib import Path
import json
import time
import threading
import requests

# Core imports
try:
    from vosk import Model, KaldiRecognizer
    from gtts import gTTS
    from openai import OpenAI
    import whisper
except ImportError as e:
    st.error(f"Missing package: {e}")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# Session state initialization
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'assistant_ready' not in st.session_state:
    st.session_state.assistant_ready = False
if 'is_recording' not in st.session_state:
    st.session_state.is_recording = False
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None
if 'chat_mode' not in st.session_state:
    st.session_state.chat_mode = "written"
if 'auto_record' not in st.session_state:
    st.session_state.auto_record = False
if 'first_message_sent' not in st.session_state:
    st.session_state.first_message_sent = False
if 'speech_recognition_method' not in st.session_state:
    st.session_state.speech_recognition_method = "openai"  # openai, whisper, or vosk
if 'vosk_model_loaded' not in st.session_state:
    st.session_state.vosk_model_loaded = False
if 'vosk_model' not in st.session_state:
    st.session_state.vosk_model = None

class ArabicVoiceAssistant:
    def __init__(self):
        """Initialize Arabic Voice Assistant"""
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        
        self.sample_rate = 16000
        
        # Pure Arabic system prompt
        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
- Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
- Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
- Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
1. Ø§Ø¬ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
2. ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
3. Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
4. Ø§Ø³Ø£Ù„ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±
5. Ø¹Ù†Ø¯ Ø­Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŒ Ø§Ø·Ù„Ø¨ÙŠ Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ±Ø­ÙŠØ¨: "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
"""

    def transcribe_audio_openai(self, audio_file_path):
        """Transcribe audio using OpenAI API"""
        try:
            with open(audio_file_path, "rb") as audio_file:
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1", 
                    file=audio_file,
                    language="ar"
                )
            return transcript.text
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª Ø¹Ø¨Ø± OpenAI: {e}")
            return ""

    def transcribe_audio_whisper(self, audio_file_path):
        """Transcribe audio using local Whisper"""
        try:
            if not hasattr(st.session_state, 'whisper_model') or st.session_state.whisper_model is None:
                with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper Ø§Ù„Ù…Ø­Ù„ÙŠ..."):
                    st.session_state.whisper_model = whisper.load_model("base")
            
            result = st.session_state.whisper_model.transcribe(audio_file_path, language="ar")
            return result["text"].strip()
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª Ø¹Ø¨Ø± Whisper Ø§Ù„Ù…Ø­Ù„ÙŠ: {e}")
            return ""

    def transcribe_audio_vosk(self, audio_file_path):
        """Transcribe audio using Vosk"""
        try:
            if not st.session_state.vosk_model_loaded:
                with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Vosk..."):
                    # Try to find Vosk model
                    search_paths = [
                        Path.cwd() / "vosk-model-ar",
                        Path.home() / "Downloads" / "vosk-model-ar",
                        Path.home() / "vosk-model-ar"
                    ]
                    
                    model_path = None
                    for path in search_paths:
                        if path.exists():
                            model_path = str(path)
                            break
                    
                    if model_path:
                        st.session_state.vosk_model = Model(model_path)
                        st.session_state.vosk_model_loaded = True
                    else:
                        st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Vosk Ø§Ù„Ø¹Ø±Ø¨ÙŠ")
                        return ""
            
            rec = KaldiRecognizer(st.session_state.vosk_model, self.sample_rate)
            
            with wave.open(audio_file_path, 'rb') as wav_file:
                # Check sample rate
                if wav_file.getframerate() != self.sample_rate:
                    st.warning("âš ï¸ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø© ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚ØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©")
                
                # Read and process audio data
                while True:
                    data = wav_file.readframes(4000)
                    if len(data) == 0:
                        break
                    if rec.AcceptWaveform(data):
                        pass
                
                result = json.loads(rec.FinalResult())
                return result.get("text", "").strip()
                
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª Ø¹Ø¨Ø± Vosk: {e}")
            return ""

    def transcribe_audio_file(self, audio_file_path):
        """Transcribe audio file using the selected method"""
        if not os.path.exists(audio_file_path):
            st.error("âŒ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            return ""
        
        method = st.session_state.speech_recognition_method
        
        if method == "openai":
            return self.transcribe_audio_openai(audio_file_path)
        elif method == "whisper":
            return self.transcribe_audio_whisper(audio_file_path)
        elif method == "vosk":
            return self.transcribe_audio_vosk(audio_file_path)
        else:
            return ""

    def generate_response(self, user_text):
        """Generate Arabic response"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Add recent chat history (last 5 messages)
            for msg in st.session_state.chat_history[-5:]:
                messages.append({"role": "user", "content": msg["user"]})
                messages.append({"role": "assistant", "content": msg["assistant"]})
            
            # Add current user message
            messages.append({"role": "user", "content": user_text})
            
            # Generate response
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def generate_greeting(self):
        """Generate a greeting message"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.append({"role": "user", "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹"})
            
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    def text_to_speech(self, text):
        """Generate Arabic TTS"""
        try:
            tts = gTTS(text=text, lang='ar', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3", delete=False)
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª: {e}")
            return None

# Initialize assistant
@st.cache_resource
def get_assistant():
    return ArabicVoiceAssistant()

def record_audio(duration=5):
    """Record audio for specified duration"""
    try:
        st.info(f"ğŸ”´ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„... ({duration} Ø«ÙˆØ§Ù†ÙŠ)")
        
        # Record audio
        audio_data = sd.rec(int(duration * 16000), 
                          samplerate=16000, 
                          channels=1, 
                          dtype='float32')
        
        # Show countdown
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i in range(duration):
            progress_bar.progress((i + 1) / duration)
            status_text.text(f"ğŸ¤ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¬Ø§Ø±ÙŠ... {duration - i} Ø«ÙˆØ§Ù†ÙŠ Ù…ØªØ¨Ù‚ÙŠØ©")
            time.sleep(1)
        
        sd.wait()
        progress_bar.empty()
        status_text.empty()
        
        # Convert to int16 and save as WAV
        audio_int16 = (audio_data * 32767).astype(np.int16)
        
        # Create temporary file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        temp_file.close()
        
        with wave.open(temp_file.name, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(16000)
            wav_file.writeframes(audio_int16.tobytes())
        
        return temp_file.name
    
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: {e}")
        return None

def play_audio(audio_file):
    """Play audio in browser"""
    if audio_file and os.path.exists(audio_file):
        try:
            with open(audio_file, "rb") as f:
                audio_bytes = f.read()
            audio_b64 = base64.b64encode(audio_bytes).decode()
            
            audio_html = f"""
            <audio controls autoplay style="width: 100%;">
                <source src="data:audio/mp3;base64,{audio_b64}" type="audio/mp3">
                Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª
            </audio>
            """
            st.markdown(audio_html, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª: {e}")

def send_greeting(assistant):
    """Send greeting message from assistant"""
    if not st.session_state.first_message_sent:
        with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¶ÙŠØ±..."):
            greeting = assistant.generate_greeting()
            
            st.session_state.chat_history.append({
                "user": "",
                "assistant": greeting
            })
            
            if st.session_state.chat_mode in ["audio", "twins"]:
                with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ­ÙŠØ© Ø¥Ù„Ù‰ ØµÙˆØª..."):
                    audio_file = assistant.text_to_speech(greeting)
                
                if audio_file and os.path.exists(audio_file):
                    st.session_state["audio_greeting"] = audio_file
                    play_audio(audio_file)
        
        st.session_state.first_message_sent = True
        st.rerun()

def main():
    """Main application"""
    
    st.title("ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†")
    st.markdown("### ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠØ© - Ø³Ø§Ù†Ø¯ÙŠ")
    st.markdown("**Ù…Ø±Ø­Ø¨Ø§Ù‹ ÙˆØ£Ù‡Ù„Ø§Ù‹ Ø¨ÙƒÙ…! ÙŠÙ…ÙƒÙ†ÙƒÙ… Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**")
    
    assistant = get_assistant()
    
    with st.sidebar:
        st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        if not st.session_state.assistant_ready:
            with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ø¶ÙŠØ±..."):
                st.session_state.assistant_ready = True
                st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø²")
        
        st.divider()
        
        st.header("ğŸ™ï¸ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª")
        recognition_method = st.radio(
            "Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª:",
            ["OpenAI API", "Whisper Ù…Ø­Ù„ÙŠ", "Vosk"],
            captions=["Ø§Ù„Ø£ÙØ¶Ù„ (ÙŠØªØ·Ù„Ø¨ Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª)", "Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©", "Ø¨Ø¯ÙŠÙ„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ"],
            index=0
        )
        
        method_map = {
            "OpenAI API": "openai",
            "Whisper Ù…Ø­Ù„ÙŠ": "whisper", 
            "Vosk": "vosk"
        }
        st.session_state.speech_recognition_method = method_map[recognition_method]
        
        st.divider()
        
        st.header("ğŸ’¬ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        chat_mode = st.radio(
            "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:",
            ["Ù†ØµÙŠ", "ØµÙˆØªÙŠ", "ØªÙˆØ£Ù…"],
            captions=["Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ© Ø¹Ø§Ø¯ÙŠØ©", "ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠ", "Ù†Øµ ÙˆØµÙˆØª Ù…Ø¹Ø§Ù‹"],
            index=0
        )
        
        mode_map = {"Ù†ØµÙŠ": "written", "ØµÙˆØªÙŠ": "audio", "ØªÙˆØ£Ù…": "twins"}
        st.session_state.chat_mode = mode_map[chat_mode]
        
        if st.session_state.chat_mode in ["audio", "twins"]:
            st.subheader("ğŸ¤ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
            record_duration = st.slider("Ù…Ø¯Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Ø«Ø§Ù†ÙŠØ©)", 3, 10, 5)
            
            if st.session_state.chat_mode == "audio":
                st.session_state.auto_record = st.checkbox("Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø±Ø¯", value=True)
        
        st.divider()
        
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st.session_state.chat_history = []
            st.session_state.first_message_sent = False
            st.rerun()
        
        st.divider()
        
        st.header("ğŸ¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©")
        st.info("""
        **Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†**
        
        ğŸ“ ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±  
        ğŸ“ (604) 555-DENTAL
        
        **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„:**
        â€¢ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†-Ø§Ù„Ø¬Ù…Ø¹Ø©: 8Øµ-6Ù…
        â€¢ Ø§Ù„Ø³Ø¨Øª: 9Øµ-3Ù…  
        â€¢ Ø§Ù„Ø£Ø­Ø¯: Ù…ØºÙ„Ù‚
        
        **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…
        â€¢ ØªÙ†Ø¸ÙŠÙ ÙˆÙØ­Øµ Ø§Ù„Ø£Ø³Ù†Ø§Ù†
        â€¢ Ø§Ù„Ø­Ø´ÙˆØ§Øª ÙˆØ§Ù„ØªÙŠØ¬Ø§Ù†
        â€¢ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ
        """)

    if st.session_state.assistant_ready and not st.session_state.first_message_sent:
        send_greeting(assistant)

    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        mode_display = {
            "written": "ğŸ“ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù†ØµÙŠ",
            "audio": "ğŸ¤ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµÙˆØªÙŠ", 
            "twins": "ğŸ‘¥ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØªÙˆØ£Ù…"
        }
        st.info(f"**Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ:** {mode_display[st.session_state.chat_mode]}")
        
        method_display = {
            "openai": "OpenAI API ğŸŒ",
            "whisper": "Whisper Ù…Ø­Ù„ÙŠ ğŸ’»",
            "vosk": "Vosk ğŸ”„"
        }
        st.info(f"**Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª:** {method_display[st.session_state.speech_recognition_method]}")
        
        if st.session_state.chat_mode in ["audio", "twins"]:
            st.subheader("ğŸ¤ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„ØµÙˆØªÙŠ")
            
            col_rec1, col_rec2 = st.columns(2)
            
            with col_rec1:
                if st.button("ğŸ™ï¸ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„", disabled=not st.session_state.assistant_ready):
                    audio_file = record_audio(record_duration)
                    if audio_file and os.path.exists(audio_file):
                        st.session_state.last_recording = audio_file
                        st.success("âœ… ØªÙ… Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
                        st.audio(audio_file, format="audio/wav")
                    else:
                        st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
            
            with col_rec2:
                if st.button("ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„") and hasattr(st.session_state, 'last_recording'):
                    if os.path.exists(st.session_state.last_recording):
                        with st.spinner("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ..."):
                            text = assistant.transcribe_audio_file(st.session_state.last_recording)
                            
                            if text:
                                st.success(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ: {text}")
                                process_user_input(assistant, text)
                            else:
                                st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù…. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
                    else:
                        st.error("âŒ Ù…Ù„Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
            
            st.subheader("ğŸ“ Ø±ÙØ¹ Ù…Ù„Ù ØµÙˆØªÙŠ")
            uploaded_file = st.file_uploader("Ø§Ø®ØªØ± Ù…Ù„Ù ØµÙˆØªÙŠ (WAV, MP3)", type=['wav', 'mp3'], key="audio_uploader")
            
            if uploaded_file and st.button("ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹", key="process_uploaded"):
                file_ext = os.path.splitext(uploaded_file.name)[1].lower()
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_ext, delete=False)
                temp_file.write(uploaded_file.read())
                temp_file.close()
                
                if os.path.exists(temp_file.name):
                    with st.spinner("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ..."):
                        text = assistant.transcribe_audio_file(temp_file.name)
                        
                        if text:
                            st.success(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ: {text}")
                            process_user_input(assistant, text)
                        else:
                            st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù… ÙÙŠ Ø§Ù„Ù…Ù„Ù.")
                    
                    try:
                        os.unlink(temp_file.name)
                    except:
                        pass
                else:
                    st.error("âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹")
        
        if st.session_state.chat_mode in ["written", "twins"]:
            st.subheader("âŒ¨ï¸ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†ØµÙŠ")
            user_text = st.text_area("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§:", height=100, 
                                    placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©ØŸ", key="text_input")
            
            if st.button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©", key="send_text") and user_text.strip():
                process_user_input(assistant, user_text.strip())
        
        st.divider()
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        for i, msg in enumerate(st.session_state.chat_history):
            if msg["user"]:
                st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {msg['user']}")
            st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
            
            if st.session_state.chat_mode in ["audio", "twins"]:
                audio_key = f"audio_{i}" if i > 0 else "audio_greeting"
                if audio_key in st.session_state and os.path.exists(st.session_state[audio_key]):
                    st.audio(st.session_state[audio_key], format="audio/mp3")
            
            st.divider()

    with col2:
        st.header("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        st.metric("ğŸ¤– Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯", "Ø¬Ø§Ù‡Ø²" if st.session_state.assistant_ready else "ØªØ­Ù…ÙŠÙ„")
        st.metric("ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„", len(st.session_state.chat_history))
        st.metric("ğŸ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", mode_display[st.session_state.chat_mode])
        st.metric("ğŸ™ï¸ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¹Ø±Ù", method_display[st.session_state.speech_recognition_method])
        
        if st.session_state.chat_history:
            last_msg = st.session_state.chat_history[-1]
            display_text = last_msg['user'] if last_msg["user"] else last_msg['assistant']
            st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø©:", value=display_text, height=100, disabled=True)
        
        st.divider()
        st.subheader("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†")
        if st.button("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†"):
            test_microphone(assistant)

def process_user_input(assistant, text):
    """Process user input and generate response"""
    with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯..."):
        response = assistant.generate_response(text)
    
    st.session_state.chat_history.append({
        "user": text,
        "assistant": response
    })
    
    if st.session_state.chat_mode in ["audio", "twins"]:
        with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ ØµÙˆØª..."):
            audio_file = assistant.text_to_speech(response)
        
        if audio_file and os.path.exists(audio_file):
            msg_index = len(st.session_state.chat_history) - 1
            st.session_state[f"audio_{msg_index}"] = audio_file
            
            st.success("ğŸ”Š ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¯...")
            play_audio(audio_file)
            
            if st.session_state.chat_mode == "audio" and st.session_state.auto_record:
                st.info("â³ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ...")
                time.sleep(3)
                st.rerun()
    
    st.rerun()

def test_microphone(assistant):
    """Test microphone"""
    try:
        st.info("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù„Ù…Ø¯Ø© 3 Ø«ÙˆØ§Ù†...")
        
        audio_data = sd.rec(int(3 * 16000), samplerate=16000, channels=1, dtype='float32')
        
        progress_bar = st.progress(0)
        for i in range(3):
            progress_bar.progress((i + 1) / 3)
            time.sleep(1)
        
        sd.wait()
        progress_bar.empty()
        
        volume = np.abs(audio_data).mean()
        
        if volume > 0.001:
            st.success(f"âœ… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯! Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª: {volume:.4f}")
            
            # Save test audio
            audio_int16 = (audio_data * 32767).astype(np.int16)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav", delete=False)
            temp_file.close()
            
            with wave.open(temp_file.name, 'wb') as wav_file:
                wav_file.setnchannels(1)
                wav_file.setsampwidth(2)
                wav_file.setframerate(16000)
                wav_file.writeframes(audio_int16.tobytes())
            
            if os.path.exists(temp_file.name):
                with st.spinner("ğŸ”Š Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…..."):
                    text = assistant.transcribe_audio_file(temp_file.name)
                    
                    if text.strip():
                        st.success(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰: '{text}'")
                    else:
                        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù… ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")
                
                try:
                    os.unlink(temp_file.name)
                except:
                    pass
        else:
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØµÙˆØª. ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.")
            
    except Exception as e:
        st.error(f"âŒ ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†: {e}")

if __name__ == "__main__":
    main()