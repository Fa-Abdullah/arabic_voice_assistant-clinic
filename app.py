"""
Arabic Voice Assistant - Online Deployment Version
Optimized for Streamlit Cloud, Hugging Face Spaces, and Google Colab
"""

import streamlit as st
import numpy as np
import tempfile
import os
import base64
import json
import time
import io
from pathlib import Path
import requests

# Core imports with fallbacks
try:
    from gtts import gTTS
    from openai import OpenAI
    import speech_recognition as sr
    from pydub import AudioSegment
except ImportError as e:
    st.error(f"Missing package: {e}")
    st.stop()

# Page configuration
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# Session state initialization
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'assistant_ready' not in st.session_state:
    st.session_state.assistant_ready = True  # Always ready for online version
if 'chat_mode' not in st.session_state:
    st.session_state.chat_mode = "hybrid"  # "text", "audio", "hybrid"
if 'first_message_sent' not in st.session_state:
    st.session_state.first_message_sent = False

class OnlineArabicVoiceAssistant:
    def __init__(self):
        """Initialize Online Arabic Voice Assistant"""
        # Check if running in different environments
        self.is_colab = 'google.colab' in str(get_ipython()) if 'get_ipython' in globals() else False
        self.is_local = not (os.environ.get('STREAMLIT_SHARING_MODE') or 
                           os.environ.get('HUGGINGFACE_HUB_CACHE'))
        
        # Initialize OpenAI client
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        
        # Initialize Speech Recognition
        self.recognizer = sr.Recognizer()
        
        # Arabic system prompt
        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
- Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
- Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
- Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
1. Ø§Ø¬ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
2. ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
3. Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
4. Ø§Ø³Ø£Ù„ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±
5. Ø¹Ù†Ø¯ Ø­Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŒ Ø§Ø·Ù„Ø¨ÙŠ Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ±Ø­ÙŠØ¨: "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
"""

    def transcribe_audio_google(self, audio_data, language='ar-SA'):
        """Transcribe audio using Google Speech Recognition"""
        try:
            if isinstance(audio_data, str) and audio_data.startswith('data:'):
                # Handle base64 audio data
                header, encoded = audio_data.split(',', 1)
                audio_bytes = base64.b64decode(encoded)
                audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))
            elif isinstance(audio_data, bytes):
                audio_segment = AudioSegment.from_file(io.BytesIO(audio_data))
            else:
                # Handle file path
                audio_segment = AudioSegment.from_file(audio_data)
            
            # Convert to WAV format for recognition
            wav_data = audio_segment.export(format="wav").read()
            audio_source = sr.AudioFile(io.BytesIO(wav_data))
            
            with audio_source as source:
                self.recognizer.adjust_for_ambient_noise(source)
                audio_recorded = self.recognizer.record(source)
            
            # Recognize speech
            text = self.recognizer.recognize_google(
                audio_recorded, 
                language=language
            )
            return text.strip()
            
        except sr.UnknownValueError:
            return ""
        except sr.RequestError as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…: {e}")
            return ""
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª: {e}")
            return ""

    def generate_response(self, user_text):
        """Generate Arabic response using AI"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Add recent chat history
            for msg in st.session_state.chat_history[-5:]:
                if msg["user"]:
                    messages.append({"role": "user", "content": msg["user"]})
                messages.append({"role": "assistant", "content": msg["assistant"]})
            
            # Add current user message
            messages.append({"role": "user", "content": user_text})
            
            # Generate response
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {str(e)[:50]}... ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def generate_greeting(self):
        """Generate greeting message"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.append({"role": "user", "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£ÙˆØ¯ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹ Ù…ÙˆØ¸ÙØ© Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„"})
            
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=150,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    def text_to_speech(self, text):
        """Convert text to speech using gTTS"""
        try:
            tts = gTTS(text=text, lang='ar', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª: {e}")
            return None

# Initialize assistant
@st.cache_resource
def get_assistant():
    return OnlineArabicVoiceAssistant()

def create_audio_recorder_html():
    """Create HTML5 audio recorder"""
    html_code = """
    <div style="text-align: center; padding: 20px; border: 2px dashed #ccc; border-radius: 10px;">
        <h3>ğŸ™ï¸ Ù…Ø³Ø¬Ù„ Ø§Ù„ØµÙˆØª</h3>
        <div id="recorder-status">Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„" Ù„Ù„Ø¨Ø¯Ø¡</div>
        <br>
        <button id="start-btn" onclick="startRecording()" 
                style="background-color: #ff4444; color: white; padding: 10px 20px; border: none; border-radius: 5px; margin: 5px;">
            ğŸ™ï¸ Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        </button>
        <button id="stop-btn" onclick="stopRecording()" disabled
                style="background-color: #888; color: white; padding: 10px 20px; border: none; border-radius: 5px; margin: 5px;">
            â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        </button>
        <button id="process-btn" onclick="processRecording()" disabled
                style="background-color: #4444ff; color: white; padding: 10px 20px; border: none; border-radius: 5px; margin: 5px;">
            ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        </button>
        <br><br>
        <audio id="audio-playback" controls style="display:none; width: 100%;"></audio>
        <div id="countdown" style="font-size: 24px; color: red; display: none;"></div>
    </div>
    
    <script>
    let mediaRecorder;
    let audioChunks = [];
    let recordingTimer;
    let audioBlob;
    
    async function startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            mediaRecorder = new MediaRecorder(stream);
            
            mediaRecorder.ondataavailable = event => {
                audioChunks.push(event.data);
            };
            
            mediaRecorder.onstop = () => {
                audioBlob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' });
                const audioUrl = URL.createObjectURL(audioBlob);
                
                const audioElement = document.getElementById('audio-playback');
                audioElement.src = audioUrl;
                audioElement.style.display = 'block';
                
                document.getElementById('process-btn').disabled = false;
                document.getElementById('recorder-status').textContent = 'ØªÙ… Ø§Ù„ØªØ³Ø¬ÙŠÙ„! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„';
            };
            
            mediaRecorder.start();
            audioChunks = [];
            
            document.getElementById('start-btn').disabled = true;
            document.getElementById('stop-btn').disabled = false;
            document.getElementById('process-btn').disabled = true;
            document.getElementById('recorder-status').textContent = 'Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„... Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ "Ø¥ÙŠÙ‚Ø§Ù" Ø¹Ù†Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡';
            
            // Auto stop after 10 seconds
            setTimeout(() => {
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    stopRecording();
                }
            }, 10000);
            
        } catch (error) {
            document.getElementById('recorder-status').textContent = 'Ø®Ø·Ø£: Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…Ø§ÙŠÙƒ. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…Ø§ÙŠÙƒ.';
        }
    }
    
    function stopRecording() {
        if (mediaRecorder && mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
            mediaRecorder.stream.getTracks().forEach(track => track.stop());
        }
        
        document.getElementById('start-btn').disabled = false;
        document.getElementById('stop-btn').disabled = true;
    }
    
    function processRecording() {
        if (audioBlob) {
            const reader = new FileReader();
            reader.onloadend = () => {
                // Send audio data to Streamlit
                window.parent.postMessage({
                    type: 'audio-data',
                    data: reader.result
                }, '*');
                
                document.getElementById('recorder-status').textContent = 'ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...';
            };
            reader.readAsDataURL(audioBlob);
        }
    }
    </script>
    """
    return html_code

def process_user_input(assistant, text, mode="text"):
    """Process user input and generate response"""
    if not text or not text.strip():
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ø£ÙŠ Ù†Øµ")
        return
    
    st.success(f"âœ… **Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ„Ù…:** {text}")
    
    # Generate AI response
    with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯..."):
        response = assistant.generate_response(text.strip())
    
    # Add to chat history
    st.session_state.chat_history.append({
        "user": text.strip(),
        "assistant": response,
        "mode": mode
    })
    
    # Generate audio response if needed
    if st.session_state.chat_mode in ["audio", "hybrid"]:
        with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ ØµÙˆØª..."):
            audio_file = assistant.text_to_speech(response)
            
            if audio_file:
                # Store audio file reference
                msg_index = len(st.session_state.chat_history) - 1
                st.session_state[f"audio_{msg_index}"] = audio_file
    
    st.rerun()

def display_chat_history():
    """Display chat history with audio playback"""
    if not st.session_state.chat_history:
        st.info("ğŸ’¬ Ø§Ø¨Ø¯Ø£ Ù…Ø­Ø§Ø¯Ø«ØªÙƒ...")
        return
    
    for i, msg in enumerate(st.session_state.chat_history):
        # User message
        if msg["user"]:
            mode_emoji = "ğŸ¤" if msg.get("mode") == "audio" else "âŒ¨ï¸"
            st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶ {mode_emoji}:** {msg['user']}")
        
        # Assistant response
        st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
        
        # Audio playback if available
        audio_key = f"audio_{i}"
        if audio_key in st.session_state and os.path.exists(st.session_state[audio_key]):
            with open(st.session_state[audio_key], "rb") as audio_file:
                audio_bytes = audio_file.read()
            st.audio(audio_bytes, format="audio/mp3")
        
        st.divider()

def send_initial_greeting(assistant):
    """Send initial greeting from assistant"""
    if not st.session_state.first_message_sent:
        with st.spinner("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹..."):
            greeting = assistant.generate_greeting()
            
            st.session_state.chat_history.append({
                "user": "",
                "assistant": greeting,
                "mode": "system"
            })
            
            # Generate audio greeting if needed
            if st.session_state.chat_mode in ["audio", "hybrid"]:
                audio_file = assistant.text_to_speech(greeting)
                if audio_file:
                    st.session_state["audio_0"] = audio_file
        
        st.session_state.first_message_sent = True
        st.rerun()

def main():
    """Main application"""
    
    # Title and header
    st.title("ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†")
    st.markdown("### ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠØ© - Ø³Ø§Ù†Ø¯ÙŠ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†)")
    st.markdown("**Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù†ØµÙŠØ§Ù‹ Ø£Ùˆ ØµÙˆØªÙŠØ§Ù‹**")
    
    # Initialize assistant
    assistant = get_assistant()
    
    # Sidebar configuration
    with st.sidebar:
        st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # System status
        st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†")
        
        # Environment info
        if assistant.is_colab:
            st.info("ğŸ”¬ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¨ÙŠØ¦Ø© Google Colab")
        elif not assistant.is_local:
            st.info("â˜ï¸ ÙŠØ¹Ù…Ù„ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")
        else:
            st.info("ğŸ’» ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹")
        
        st.divider()
        
        # Chat mode selection
        st.header("ğŸ’¬ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        chat_mode = st.selectbox(
            "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„ØªÙØ§Ø¹Ù„:",
            ["Ù†ØµÙŠ", "ØµÙˆØªÙŠ", "Ù…Ø®ØªÙ„Ø·"],
            index=2,
            help="Ø§Ù„Ù†ØµÙŠ: ÙƒØªØ§Ø¨Ø© ÙÙ‚Ø·ØŒ Ø§Ù„ØµÙˆØªÙŠ: ØªØ³Ø¬ÙŠÙ„ ÙÙ‚Ø·ØŒ Ø§Ù„Ù…Ø®ØªÙ„Ø·: ÙƒÙ„Ø§Ù‡Ù…Ø§"
        )
        
        # Map selection to internal mode
        mode_map = {"Ù†ØµÙŠ": "text", "ØµÙˆØªÙŠ": "audio", "Ù…Ø®ØªÙ„Ø·": "hybrid"}
        st.session_state.chat_mode = mode_map[chat_mode]
        
        st.divider()
        
        # Clear chat
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st
