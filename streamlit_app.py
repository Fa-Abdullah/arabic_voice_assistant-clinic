import streamlit as st
import numpy as np
import tempfile
import os
import base64
import time
import io
import sounddevice as sd
from pathlib import Path

# Core imports
try:
    from gtts import gTTS
    from openai import OpenAI
    import speech_recognition as sr
    from pydub import AudioSegment
    from pydub.playback import play
except ImportError as e:
    st.error(f"Missing package: {e}")
    st.stop()

# Page config
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# Session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'assistant_ready' not in st.session_state:
    st.session_state.assistant_ready = True
if 'chat_mode' not in st.session_state:
    st.session_state.chat_mode = "hybrid"  # "text", "audio", "hybrid"
if 'first_message_sent' not in st.session_state:
    st.session_state.first_message_sent = False

# --------- Assistant Class ----------
class OnlineArabicVoiceAssistant:
    def __init__(self):
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        self.recognizer = sr.Recognizer()
        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:

* Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
* Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
* Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
* Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:

* Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
* ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
* Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
"""

    def transcribe_audio_google(self, audio_data, language='ar-SA'):
        try:
            if isinstance(audio_data, str) and audio_data.startswith('data:'):
                header, encoded = audio_data.split(',', 1)
                audio_bytes = base64.b64decode(encoded)
                audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))
            elif isinstance(audio_data, bytes):
                audio_segment = AudioSegment.from_file(io.BytesIO(audio_data))
            else:
                audio_segment = AudioSegment.from_file(audio_data)

            wav_data = audio_segment.export(format="wav").read()
            audio_source = sr.AudioFile(io.BytesIO(wav_data))

            with audio_source as source:
                self.recognizer.adjust_for_ambient_noise(source)
                audio_recorded = self.recognizer.record(source)

            text = self.recognizer.recognize_google(audio_recorded, language=language)
            return text.strip()
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª: {str(e)}")
            return ""

    def generate_response(self, user_text):
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø¢Ø®Ø± 5 Ø±Ø³Ø§Ø¦Ù„)
            for msg in st.session_state.chat_history[-5:]:
                if msg["user"]:
                    messages.append({"role": "user", "content": msg["user"]})
                    messages.append({"role": "assistant", "content": msg["assistant"]})
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            messages.append({"role": "user", "content": user_text})

            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"

    def generate_greeting(self):
        return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    def text_to_speech(self, text):
        try:
            tts = gTTS(text=text, lang='ar', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…: {str(e)}")
            return None

@st.cache_resource
def get_assistant():
    return OnlineArabicVoiceAssistant()

# --------- Helper Functions ----------
def process_user_input(assistant, text, mode="text"):
    if not text.strip():
        return
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„
    st.session_state.chat_history.append({"user": text.strip(), "assistant": "... Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¯", "mode": mode})
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
    response = assistant.generate_response(text.strip())
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
    st.session_state.chat_history[-1]["assistant"] = response
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆØ¶Ø¹ ØµÙˆØªÙŠ Ø£Ùˆ Ù…Ø®ØªÙ„Ø·
    if st.session_state.chat_mode in ["audio", "hybrid"]:
        audio_file = assistant.text_to_speech(response)
        if audio_file:
            st.session_state[f"audio_{len(st.session_state.chat_history)-1}"] = audio_file
    
    st.rerun()

def display_chat_history():
    for i, msg in enumerate(st.session_state.chat_history):
        if msg["user"]:
            st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {msg['user']}")
            st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
            
            # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
            audio_key = f"audio_{i}"
            if audio_key in st.session_state and os.path.exists(st.session_state[audio_key]):
                with open(st.session_state[audio_key], "rb") as f:
                    audio_bytes = f.read()
                    st.audio(audio_bytes, format="audio/mp3")
            
            st.divider()

def test_microphone():
    try:
        st.info("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù„Ù…Ø¯Ø© 3 Ø«ÙˆØ§Ù†...")
        audio_data = sd.rec(int(3*16000), samplerate=16000, channels=1, dtype='float32')
        sd.wait()
        volume = np.abs(audio_data).mean()
        if volume > 0.001:
            st.success(f"âœ… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØ¹Ù…Ù„! Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª: {volume:.4f}")
        else:
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØµÙˆØª.")
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†: {e}")

def record_audio(assistant):
    try:
        st.info("ğŸ™ï¸ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª... ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù† (5 Ø«ÙˆØ§Ù†)")
        audio_data = sd.rec(int(5 * 16000), samplerate=16000, channels=1, dtype='int16')
        sd.wait()
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø¤Ù‚Øª
        temp_audio = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        import scipy.io.wavfile as wav
        wav.write(temp_audio.name, 16000, audio_data)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ
        text = assistant.transcribe_audio_google(temp_audio.name)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        os.unlink(temp_audio.name)
        
        return text
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„: {e}")
        return ""

# --------- Main App ----------
def main():
    assistant = get_assistant()
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙØ§Ø±ØºØ©
    if not st.session_state.chat_history:
        greeting = assistant.generate_greeting()
        st.session_state.chat_history.append({"user": "", "assistant": greeting, "mode": "system"})
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¥Ù„Ù‰ ØµÙˆØª
        if st.session_state.chat_mode in ["audio", "hybrid"]:
            audio_file = assistant.text_to_speech(greeting)
            if audio_file:
                st.session_state["audio_0"] = audio_file

    col1, col2 = st.columns([2, 1])

    with col1:
        st.header("ğŸ¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©")
        st.info("""
        **Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†**

        ğŸ“ ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±  
        ğŸ“ (604) 555-DENTAL  

        **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„:**  
        â€¢ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†-Ø§Ù„Ø¬Ù…Ø¹Ø©: 8Øµ-6Ù…  
        â€¢ Ø§Ù„Ø³Ø¨Øª: 9Øµ-3Ù…  
        â€¢ Ø§Ù„Ø£Ø­Ø¯: Ù…ØºÙ„Ù‚  

        **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**  
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…  
        â€¢ ØªÙ†Ø¸ÙŠÙ ÙˆÙØ­Øµ Ø§Ù„Ø£Ø³Ù†Ø§Ù†  
        â€¢ Ø§Ù„Ø­Ø´ÙˆØ§Øª ÙˆØ§Ù„ØªÙŠØ¬Ø§Ù†  
        â€¢ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±  
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ
        """)

        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        chat_mode = st.radio(
            "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:",
            ["Ù†ØµÙŠ", "ØµÙˆØªÙŠ", "Ù…Ø®ØªÙ„Ø·"],
            horizontal=True,
            index=2
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®ÙŠØ§Ø± Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ù†Ø§Ø³Ø¨Ø©
        mode_map = {"Ù†ØµÙŠ": "text", "ØµÙˆØªÙŠ": "audio", "Ù…Ø®ØªÙ„Ø·": "hybrid"}
        st.session_state.chat_mode = mode_map[chat_mode]
        
        # ÙˆØ§Ø¬Ù‡Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ
        user_text = st.text_area("âœï¸ Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ:", height=100, placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©ØŸ")
        
        # Ø²Ø± Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù†Øµ
        if st.button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ù†Øµ"):
            process_user_input(assistant, user_text, mode="text")
        
        # Ø²Ø± Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ
        if st.button("ğŸ™ï¸ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ") and st.session_state.chat_mode in ["audio", "hybrid"]:
            recorded_text = record_audio(assistant)
            if recorded_text:
                process_user_input(assistant, recorded_text, mode="audio")
        
        st.divider()
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        display_chat_history()

    with col2:
        st.header("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        st.metric("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¬Ø§Ù‡Ø²" if st.session_state.assistant_ready else "ØªØ­Ù…ÙŠÙ„")
        st.metric("ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„", len(st.session_state.chat_history))
        st.metric("ğŸ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", st.session_state.chat_mode)

        if st.session_state.chat_history:
            last_msg = st.session_state.chat_history[-1]
            if last_msg["user"]:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø§Ù„Ù…Ø±ÙŠØ¶):", value=last_msg['user'], height=100, disabled=True)
            else:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø³Ø§Ù†Ø¯ÙŠ):", value=last_msg['assistant'], height=100, disabled=True)

        st.divider()
        st.subheader("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†")
        if st.button("ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"):
            test_microphone()
            
        st.divider()
        st.subheader("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st.session_state.chat_history = []
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
            greeting = assistant.generate_greeting()
            st.session_state.chat_history.append({"user": "", "assistant": greeting, "mode": "system"})
            st.rerun()

if __name__ == "__main__":
    main()
