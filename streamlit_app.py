import streamlit as st
import numpy as np
import tempfile
import os
import base64
import time
import io
import sounddevice as sd
from pathlib import Path

# Core imports

try:
  from gtts import gTTS
  from openai import OpenAI
  import speech\_recognition as sr
  from pydub import AudioSegment
except ImportError as e:
  st.error(f"Missing package: {e}")
  st.stop()

# Page config

st.set\_page\_config(
  page\_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
  page\_icon="ğŸ¦·",
  layout="wide"
)

# Session state

if 'chat\_history' not in st.session\_state:
  st.session\_state.chat\_history = \[]
if 'assistant\_ready' not in st.session\_state:
  st.session\_state.assistant\_ready = True
if 'chat\_mode' not in st.session\_state:
  st.session\_state.chat\_mode = "hybrid"  # "text", "audio", "hybrid"
if 'first\_message\_sent' not in st.session\_state:
  st.session\_state.first\_message\_sent = False

# --------- Assistant Class ----------

class OnlineArabicVoiceAssistant:
  def **init**(self):
    self.openai\_client = OpenAI(
      base\_url="[https://openrouter.ai/api/v1](https://openrouter.ai/api/v1)",
      api\_key="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"
    )
    self.recognizer = sr.Recognizer()
    self.system\_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

    Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:

    * Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
    * Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
    * Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
    * Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

    Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:

    * Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
    * ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
    * Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
    """

  def transcribe\_audio\_google(self, audio\_data, language='ar-SA'):
    try:
      if isinstance(audio\_data, str) and audio\_data.startswith('data:'):
        header, encoded = audio\_data.split(',', 1)
        audio\_bytes = base64.b64decode(encoded)
        audio\_segment = AudioSegment.from\_file(io.BytesIO(audio\_bytes))
      elif isinstance(audio\_data, bytes):
        audio\_segment = AudioSegment.from\_file(io.BytesIO(audio\_data))
      else:
        audio\_segment = AudioSegment.from\_file(audio\_data)

  ```
        wav_data = audio_segment.export(format="wav").read()
        audio_source = sr.AudioFile(io.BytesIO(wav_data))

        with audio_source as source:
            self.recognizer.adjust_for_ambient_noise(source)
            audio_recorded = self.recognizer.record(source)

        text = self.recognizer.recognize_google(audio_recorded, language=language)
        return text.strip()
    except:
        return ""
  ```

  def generate\_response(self, user\_text):
    try:
      messages = \[{"role": "system", "content": self.system\_prompt}]
      for msg in st.session\_state.chat\_history\[-5:]:
      if msg\["user"]:
      messages.append({"role": "user", "content": msg\["user"]})
      messages.append({"role": "assistant", "content": msg\["assistant"]})
      messages.append({"role": "user", "content": user\_text})

    ```
        response = self.openai_client.chat.completions.create(
            model="google/gemma-2-9b-it",
            messages=messages,
            max_tokens=200,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"
  ```

  def generate\_greeting(self):
    return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

  def text\_to\_speech(self, text):
    try:
      tts = gTTS(text=text, lang='ar', slow=False)
      temp\_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
      tts.save(temp\_file.name)
      return temp\_file.name
    except:
      return None

  @st.cache\_resource
  def get\_assistant():
    return OnlineArabicVoiceAssistant()

# --------- Helper Functions ----------

  def process\_user\_input(assistant, text, mode="text"):
    if not text.strip():
      return
    response = assistant.generate\_response(text.strip())
    st.session\_state.chat\_history.append({"user": text.strip(), "assistant": response, "mode": mode})
    if st.session\_state.chat\_mode in \["audio", "hybrid"]:
      audio\_file = assistant.text\_to\_speech(response)
    if audio\_file:
      st.session\_state\[f"audio\_{len(st.session\_state.chat\_history)-1}"] = audio\_file
      st.rerun()

  def display\_chat\_history():
    for i, msg in enumerate(st.session\_state.chat\_history):
      if msg\["user"]:
        st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {msg\['user']}")
        st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg\['assistant']}")
        audio\_key = f"audio\_{i}"
      if audio\_key in st.session\_state and os.path.exists(st.session\_state\[audio\_key]):
        with open(st.session\_state\[audio\_key], "rb") as f:
        st.audio(f.read(), format="audio/mp3")
        st.divider()

  def test\_microphone():
    try:
      st.info("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù„Ù…Ø¯Ø© 3 Ø«ÙˆØ§Ù†...")
      audio\_data = sd.rec(int(3\*16000), samplerate=16000, channels=1, dtype='float32')
      sd.wait()
      volume = np.abs(audio\_data).mean()
    if volume > 0.001:
      st.success(f"âœ… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØ¹Ù…Ù„! Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª: {volume:.4f}")
    else:
st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØµÙˆØª.")
except Exception as e:
st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†: {e}")

# --------- Main App ----------

def main():
assistant = get\_assistant()

```
col1, col2 = st.columns([2,1])

with col1:
    st.header("ğŸ¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©")
    st.info("""
    **Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†**

    ğŸ“ ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±  
    ğŸ“ (604) 555-DENTAL  

    **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„:**  
    â€¢ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†-Ø§Ù„Ø¬Ù…Ø¹Ø©: 8Øµ-6Ù…  
    â€¢ Ø§Ù„Ø³Ø¨Øª: 9Øµ-3Ù…  
    â€¢ Ø§Ù„Ø£Ø­Ø¯: Ù…ØºÙ„Ù‚  

    **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**  
    â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…  
    â€¢ ØªÙ†Ø¸ÙŠÙ ÙˆÙØ­Øµ Ø§Ù„Ø£Ø³Ù†Ø§Ù†  
    â€¢ Ø§Ù„Ø­Ø´ÙˆØ§Øª ÙˆØ§Ù„ØªÙŠØ¬Ø§Ù†  
    â€¢ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±  
    â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ
    """)

    st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
    user_text = st.text_area("âœï¸ Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ:", height=100, placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©ØŸ")
    if st.button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„"):
        process_user_input(assistant, user_text, mode="text")

    st.divider()
    st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
    display_chat_history()

with col2:
    st.header("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
    st.metric("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¬Ø§Ù‡Ø²" if st.session_state.assistant_ready else "ØªØ­Ù…ÙŠÙ„")
    st.metric("ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„", len(st.session_state.chat_history))
    st.metric("ğŸ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", st.session_state.chat_mode)

    if st.session_state.chat_history:
        last_msg = st.session_state.chat_history[-1]
        if last_msg["user"]:
            st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø§Ù„Ù…Ø±ÙŠØ¶):", value=last_msg['user'], height=100, disabled=True)
        else:
            st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø³Ø§Ù†Ø¯ÙŠ):", value=last_msg['assistant'], height=100, disabled=True)

    st.divider()
    st.subheader("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†")
    if st.button("ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"):
        test_microphone()
```

if **name** == "**main**":
main()
