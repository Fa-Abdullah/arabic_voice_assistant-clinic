"""
Arabic Voice Assistant - Online Deployment Version
Optimized for Streamlit Cloud, Hugging Face Spaces, and Google Colab
"""

import streamlit as st
import numpy as np
import tempfile
import os
import base64
import json
import time
import io
from pathlib import Path
import requests

# Core imports with fallbacks
try:
    from gtts import gTTS
    from openai import OpenAI
    import speech_recognition as sr
    from pydub import AudioSegment
except ImportError as e:
    st.error(f"Missing package: {e}")
    st.stop()

# Try to import sounddevice (optional)
try:
    import sounddevice as sd
    SOUNDDEVICE_AVAILABLE = True
except ImportError:
    SOUNDDEVICE_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# Session state initialization
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'assistant_ready' not in st.session_state:
    st.session_state.assistant_ready = True  # Always ready for online version
if 'chat_mode' not in st.session_state:
    st.session_state.chat_mode = "hybrid"  # "text", "audio", "hybrid"
if 'first_message_sent' not in st.session_state:
    st.session_state.first_message_sent = False

class OnlineArabicVoiceAssistant:
    def __init__(self):
        """Initialize Online Arabic Voice Assistant"""
        # Check if running in different environments
        self.is_colab = 'google.colab' in str(get_ipython()) if 'get_ipython' in globals() else False
        self.is_local = not (os.environ.get('STREAMLIT_SHARING_MODE') or 
                           os.environ.get('HUGGINGFACE_HUB_CACHE'))
        
        # Initialize OpenAI client
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        
        # Initialize Speech Recognition
        self.recognizer = sr.Recognizer()
        
        # Arabic system prompt
        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
- Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
- Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
- Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
1. Ø§Ø¬ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
2. ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
3. Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
4. Ø§Ø³Ø£Ù„ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±
5. Ø¹Ù†Ø¯ Ø­Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŒ Ø§Ø·Ù„Ø¨ÙŠ Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ±Ø­ÙŠØ¨: "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
"""

    def transcribe_audio_google(self, audio_data, language='ar-SA'):
        """Transcribe audio using Google Speech Recognition"""
        try:
            if isinstance(audio_data, str) and audio_data.startswith('data:'):
                # Handle base64 audio data
                header, encoded = audio_data.split(',', 1)
                audio_bytes = base64.b64decode(encoded)
                audio_segment = AudioSegment.from_file(io.BytesIO(audio_bytes))
            elif isinstance(audio_data, bytes):
                audio_segment = AudioSegment.from_file(io.BytesIO(audio_data))
            else:
                # Handle file path
                audio_segment = AudioSegment.from_file(audio_data)
            
            # Convert to WAV format for recognition
            wav_data = audio_segment.export(format="wav").read()
            audio_source = sr.AudioFile(io.BytesIO(wav_data))
            
            with audio_source as source:
                self.recognizer.adjust_for_ambient_noise(source)
                audio_recorded = self.recognizer.record(source)
            
            # Recognize speech
            text = self.recognizer.recognize_google(
                audio_recorded, 
                language=language
            )
            return text.strip()
            
        except sr.UnknownValueError:
            return ""
        except sr.RequestError as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…: {e}")
            return ""
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª: {e}")
            return ""

    def generate_response(self, user_text):
        """Generate Arabic response using AI"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Add recent chat history
            for msg in st.session_state.chat_history[-5:]:
                if msg["user"]:
                    messages.append({"role": "user", "content": msg["user"]})
                messages.append({"role": "assistant", "content": msg["assistant"]})
            
            # Add current user message
            messages.append({"role": "user", "content": user_text})
            
            # Generate response
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {str(e)[:50]}... ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def generate_greeting(self):
        """Generate greeting message"""
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            messages.append({"role": "user", "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£ÙˆØ¯ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹ Ù…ÙˆØ¸ÙØ© Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„"})
            
            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=150,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    def text_to_speech(self, text):
        """Convert text to speech using gTTS"""
        try:
            tts = gTTS(text=text, lang='ar', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª: {e}")
            return None

# Initialize assistant
@st.cache_resource
def get_assistant():
    return OnlineArabicVoiceAssistant()

def process_user_input(assistant, text, mode="text"):
    """Process user input and generate response"""
    if not text or not text.strip():
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ø£ÙŠ Ù†Øµ")
        return
    
    st.success(f"âœ… **Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ„Ù…:** {text}")
    
    # Generate AI response
    with st.spinner("ğŸ¤– Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯..."):
        response = assistant.generate_response(text.strip())
    
    # Add to chat history
    st.session_state.chat_history.append({
        "user": text.strip(),
        "assistant": response,
        "mode": mode
    })
    
    # Generate audio response if needed
    if st.session_state.chat_mode in ["audio", "hybrid"]:
        with st.spinner("ğŸ”Š Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø¥Ù„Ù‰ ØµÙˆØª..."):
            audio_file = assistant.text_to_speech(response)
            
            if audio_file:
                # Store audio file reference
                msg_index = len(st.session_state.chat_history) - 1
                st.session_state[f"audio_{msg_index}"] = audio_file
    
    st.rerun()

def display_chat_history():
    """Display chat history with audio playback"""
    if not st.session_state.chat_history:
        st.info("ğŸ’¬ Ø§Ø¨Ø¯Ø£ Ù…Ø­Ø§Ø¯Ø«ØªÙƒ...")
        return
    
    for i, msg in enumerate(st.session_state.chat_history):
        # User message
        if msg["user"]:
            mode_emoji = "ğŸ¤" if msg.get("mode") == "audio" else "âŒ¨ï¸"
            st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶ {mode_emoji}:** {msg['user']}")
        
        # Assistant response
        st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
        
        # Audio playback if available
        audio_key = f"audio_{i}"
        if audio_key in st.session_state and os.path.exists(st.session_state[audio_key]):
            with open(st.session_state[audio_key], "rb") as audio_file:
                audio_bytes = audio_file.read()
            st.audio(audio_bytes, format="audio/mp3")
        
        st.divider()

def send_initial_greeting(assistant):
    """Send initial greeting from assistant"""
    if not st.session_state.first_message_sent:
        with st.spinner("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹..."):
            greeting = assistant.generate_greeting()
            
            st.session_state.chat_history.append({
                "user": "",
                "assistant": greeting,
                "mode": "system"
            })
            
            # Generate audio greeting if needed
            if st.session_state.chat_mode in ["audio", "hybrid"]:
                audio_file = assistant.text_to_speech(greeting)
                if audio_file:
                    st.session_state["audio_0"] = audio_file
        
        st.session_state.first_message_sent = True
        st.rerun()

def test_microphone():
    """Test microphone functionality"""
    if not SOUNDDEVICE_AVAILABLE:
        st.error("âŒ Microphone testing requires the sounddevice package. Install it with: pip install sounddevice")
        return
    
    try:
        st.info("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù„Ù…Ø¯Ø© 3 Ø«ÙˆØ§Ù†...")
        audio_data = sd.rec(int(3*16000), samplerate=16000, channels=1, dtype='float32')
        sd.wait()
        volume = np.abs(audio_data).mean()
        if volume > 0.001:
            st.success(f"âœ… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙŠØ¹Ù…Ù„! Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙˆØª: {volume:.4f}")
        else:
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ØµÙˆØª.")
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†: {e}")

def main():
    """Main application"""
    
    # Title and header
    st.title("ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†")
    st.markdown("### ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø°ÙƒÙŠØ© - Ø³Ø§Ù†Ø¯ÙŠ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†)")
    st.markdown("**Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù†ØµÙŠØ§Ù‹ Ø£Ùˆ ØµÙˆØªÙŠØ§Ù‹**")
    
    # Initialize assistant
    assistant = get_assistant()
    
    # Sidebar configuration
    with st.sidebar:
        st.header("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        # System status
        st.success("âœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†")
        
        # Environment info
        if assistant.is_colab:
            st.info("ğŸ”¬ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¨ÙŠØ¦Ø© Google Colab")
        elif not assistant.is_local:
            st.info("â˜ï¸ ÙŠØ¹Ù…Ù„ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø³Ø­Ø§Ø¨Ø©")
        else:
            st.info("ğŸ’» ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹")
        
        st.divider()
        
        # Chat mode selection
        st.header("ğŸ’¬ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        chat_mode = st.selectbox(
            "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„ØªÙØ§Ø¹Ù„:",
            ["Ù†ØµÙŠ", "ØµÙˆØªÙŠ", "Ù…Ø®ØªÙ„Ø·"],
            index=2,
            help="Ø§Ù„Ù†ØµÙŠ: ÙƒØªØ§Ø¨Ø© ÙÙ‚Ø·ØŒ Ø§Ù„ØµÙˆØªÙŠ: ØªØ³Ø¬ÙŠÙ„ ÙÙ‚Ø·ØŒ Ø§Ù„Ù…Ø®ØªÙ„Ø·: ÙƒÙ„Ø§Ù‡Ù…Ø§"
        )
        
        # Map selection to internal mode
        mode_map = {"Ù†ØµÙŠ": "text", "ØµÙˆØªÙŠ": "audio", "Ù…Ø®ØªÙ„Ø·": "hybrid"}
        st.session_state.chat_mode = mode_map[chat_mode]
        
        st.divider()
        
        # Clear chat
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st.session_state.chat_history = []
            st.session_state.first_message_sent = False
            st.rerun()
        
        st.divider()
        
        # Clinic info
        st.header("ğŸ¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©")
        st.info("""
        **Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†**
        
        ğŸ“ ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±  
        ğŸ“ (604) 555-DENTAL
        
        **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„:**
        â€¢ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†-Ø§Ù„Ø¬Ù…Ø¹Ø©: 8Øµ-6Ù…
        â€¢ Ø§Ù„Ø³Ø¨Øª: 9Øµ-3Ù…  
        â€¢ Ø§Ù„Ø£Ø­Ø¯: Ù…ØºÙ„Ù‚
        
        **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…
        â€¢ ØªÙ†Ø¸ÙŠÙ ÙˆÙØ­Øµ Ø§Ù„Ø£Ø³Ù†Ø§Ù†
        â€¢ Ø§Ù„Ø­Ø´ÙˆØ§Øª ÙˆØ§Ù„ØªÙŠØ¬Ø§Ù†
        â€¢ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ
        """)

    # Send greeting message if not already sent
    if not st.session_state.first_message_sent:
        send_initial_greeting(assistant)

    # Main content
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        # Display current mode
        mode_display = {
            "text": "ğŸ“ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù†ØµÙŠ",
            "audio": "ğŸ¤ Ø§Ù„Ù†Ù…Ø· Ø§Ù„ØµÙˆØªÙŠ",
            "hybrid": "ğŸ‘¥ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø®ØªÙ„Ø· (Ù†ØµÙŠ ÙˆØµÙˆØªÙŠ)"
        }
        st.info(f"**Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ:** {mode_display[st.session_state.chat_mode]}")
        
        # Text input for written and hybrid modes
        if st.session_state.chat_mode in ["text", "hybrid"]:
            st.subheader("âŒ¨ï¸ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†ØµÙŠ")
            user_text = st.text_area("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§:", height=100, 
                                    placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©ØŸ", key="text_input")
            
            if st.button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©", key="send_text") and user_text.strip():
                process_user_input(assistant, user_text.strip(), "text")
        
        # Chat history
        st.divider()
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        display_chat_history()

    with col2:
        st.header("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        
        st.metric("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¬Ø§Ù‡Ø²" if st.session_state.assistant_ready else "ØªØ­Ù…ÙŠÙ„")
        st.metric("ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„", len(st.session_state.chat_history))
        st.metric("ğŸ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", mode_display[st.session_state.chat_mode])
        
        if st.session_state.chat_history:
            last_msg = st.session_state.chat_history[-1]
            if last_msg["user"]:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø§Ù„Ù…Ø±ÙŠØ¶):", value=last_msg['user'], height=100, disabled=True)
            else:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø³Ø§Ù†Ø¯ÙŠ):", value=last_msg['assistant'], height=100, disabled=True)
        
        # Microphone test
        st.divider()
        st.subheader("ğŸ¤ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†")
        if st.button("Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†"):
            test_microphone()

if __name__ == "__main__":
    main()
