import streamlit as st
import numpy as np
import tempfile
import os
import base64
import time
import io
from pathlib import Path

# Core imports
try:
    from gtts import gTTS
    from openai import OpenAI
    from pydub import AudioSegment
    import streamlit_mic_recorder as smr  # Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ù…ØªØµÙØ­
except ImportError as e:
    st.error(f"Missing package: {e}")
    st.stop()

# Page config
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# Session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'assistant_ready' not in st.session_state:
    st.session_state.assistant_ready = True
if 'chat_mode' not in st.session_state:
    st.session_state.chat_mode = "hybrid"  # "text", "audio", "hybrid"
if 'first_message_sent' not in st.session_state:
    st.session_state.first_message_sent = False
if 'audio_data' not in st.session_state:
    st.session_state.audio_data = None

# --------- Assistant Class ----------
class OnlineArabicVoiceAssistant:
    def __init__(self):
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:

* Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
* Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
* Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
* Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:

* Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
* ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
* Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
"""

    def transcribe_audio_google(self, audio_bytes, language='ar-SA'):
        try:
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmpfile:
                tmpfile.write(audio_bytes)
                tmpfile.flush()
                
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… pydub Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨
                audio = AudioSegment.from_file(tmpfile.name)
                audio = audio.set_channels(1).set_frame_rate(16000)
                
                # Ø­ÙØ¸ Ø§Ù„ØµÙˆØª Ø§Ù„Ù…Ø­ÙˆÙ„
                converted_audio = io.BytesIO()
                audio.export(converted_audio, format="wav")
                converted_audio.seek(0)
                
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Whisper Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1", 
                    file=("audio.wav", converted_audio.read()),
                    language=language
                )
                return transcript.text.strip()
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª: {str(e)}")
            return ""

    def generate_response(self, user_text):
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ø¢Ø®Ø± 5 Ø±Ø³Ø§Ø¦Ù„)
            for msg in st.session_state.chat_history[-5:]:
                if msg["user"]:
                    messages.append({"role": "user", "content": msg["user"]})
                    messages.append({"role": "assistant", "content": msg["assistant"]})
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            messages.append({"role": "user", "content": user_text})

            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"

    def generate_greeting(self):
        return "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†. Ø§Ø³Ù…ÙŠ Ø³Ø§Ù†Ø¯ÙŠØŒ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"

    def text_to_speech(self, text):
        try:
            tts = gTTS(text=text, lang='ar', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…: {str(e)}")
            return None

@st.cache_resource
def get_assistant():
    return OnlineArabicVoiceAssistant()

# --------- Helper Functions ----------
def process_user_input(assistant, text, mode="text"):
    if not text.strip():
        return
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„
    st.session_state.chat_history.append({"user": text.strip(), "assistant": "... Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±Ø¯", "mode": mode})
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
    response = assistant.generate_response(text.strip())
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
    st.session_state.chat_history[-1]["assistant"] = response
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆØ¶Ø¹ ØµÙˆØªÙŠ Ø£Ùˆ Ù…Ø®ØªÙ„Ø·
    if st.session_state.chat_mode in ["audio", "hybrid"]:
        audio_file = assistant.text_to_speech(response)
        if audio_file:
            st.session_state[f"audio_{len(st.session_state.chat_history)-1}"] = audio_file
    
    st.rerun()

def display_chat_history():
    for i, msg in enumerate(st.session_state.chat_history):
        if msg["user"]:
            st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {msg['user']}")
            st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
            
            # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØª Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹
            audio_key = f"audio_{i}"
            if audio_key in st.session_state and os.path.exists(st.session_state[audio_key]):
                with open(st.session_state[audio_key], "rb") as f:
                    audio_bytes = f.read()
                    st.audio(audio_bytes, format="audio/mp3")
            
            st.divider()

def process_recorded_audio(assistant, audio_bytes):
    if audio_bytes:
        # Ø¹Ø±Ø¶ Ù…Ø¤Ø´Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª..."):
            text = assistant.transcribe_audio_google(audio_bytes)
        
        if text:
            st.success(f"ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ: {text}")
            process_user_input(assistant, text, mode="audio")
        else:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø£ÙŠ ÙƒÙ„Ø§Ù… ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„")

# --------- Main App ----------
def main():
    assistant = get_assistant()
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© ØªØ±Ø­ÙŠØ¨ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙØ§Ø±ØºØ©
    if not st.session_state.chat_history:
        greeting = assistant.generate_greeting()
        st.session_state.chat_history.append({"user": "", "assistant": greeting, "mode": "system"})
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¥Ù„Ù‰ ØµÙˆØª
        if st.session_state.chat_mode in ["audio", "hybrid"]:
            audio_file = assistant.text_to_speech(greeting)
            if audio_file:
                st.session_state["audio_0"] = audio_file

    col1, col2 = st.columns([2, 1])

    with col1:
        st.header("ğŸ¥ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©")
        st.info("""
        **Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†**

        ğŸ“ ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±  
        ğŸ“ (604) 555-DENTAL  

        **Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„:**  
        â€¢ Ø§Ù„Ø§Ø«Ù†ÙŠÙ†-Ø§Ù„Ø¬Ù…Ø¹Ø©: 8Øµ-6Ù…  
        â€¢ Ø§Ù„Ø³Ø¨Øª: 9Øµ-3Ù…  
        â€¢ Ø§Ù„Ø£Ø­Ø¯: Ù…ØºÙ„Ù‚  

        **Ø§Ù„Ø®Ø¯Ù…Ø§Øª:**  
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…  
        â€¢ ØªÙ†Ø¸ÙŠÙ ÙˆÙØ­Øµ Ø§Ù„Ø£Ø³Ù†Ø§Ù†  
        â€¢ Ø§Ù„Ø­Ø´ÙˆØ§Øª ÙˆØ§Ù„ØªÙŠØ¬Ø§Ù†  
        â€¢ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±  
        â€¢ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ
        """)

        st.header("ğŸ’¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        
        # Ø®ÙŠØ§Ø±Ø§Øª Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        chat_mode = st.radio(
            "Ø§Ø®ØªØ± Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:",
            ["Ù†ØµÙŠ", "ØµÙˆØªÙŠ", "Ù…Ø®ØªÙ„Ø·"],
            horizontal=True,
            index=2
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®ÙŠØ§Ø± Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ù†Ø§Ø³Ø¨Ø©
        mode_map = {"Ù†ØµÙŠ": "text", "ØµÙˆØªÙŠ": "audio", "Ù…Ø®ØªÙ„Ø·": "hybrid"}
        st.session_state.chat_mode = mode_map[chat_mode]
        
        # ÙˆØ§Ø¬Ù‡Ø© Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù†Øµ
        user_text = st.text_area("âœï¸ Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ:", height=100, placeholder="Ù…Ø«Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø¹ÙŠØ§Ø¯Ø©ØŸ")
        
        # Ø²Ø± Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù†Øµ
        if st.button("ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ù†Øµ"):
            process_user_input(assistant, user_text, mode="text")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ù…ØªØµÙØ­
        st.subheader("ğŸ¤ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØªÙŠ")
        st.write("Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± Ù„Ù„ØªØ³Ø¬ÙŠÙ„ Ø«Ù… ØªÙˆÙ‚Ù Ø¹Ù†Ø¯Ù…Ø§ ØªÙ†ØªÙ‡ÙŠ")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒÙˆÙ† ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù…Ù† streamlit-mic-recorder
        audio_bytes = None
        if st.session_state.chat_mode in ["audio", "hybrid"]:
            audio_bytes = smr.mic_recorder(
                start_prompt="âºï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„",
                stop_prompt="â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„",
                key="recorder"
            )
            
            if audio_bytes and audio_bytes.get("bytes"):
                process_recorded_audio(assistant, audio_bytes["bytes"])
        
        st.divider()
        st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        display_chat_history()

    with col2:
        st.header("ğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…")
        st.metric("ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¬Ø§Ù‡Ø²" if st.session_state.assistant_ready else "ØªØ­Ù…ÙŠÙ„")
        st.metric("ğŸ’¬ Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„", len(st.session_state.chat_history))
        st.metric("ğŸ¯ Ù†Ù…Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©", st.session_state.chat_mode)

        if st.session_state.chat_history:
            last_msg = st.session_state.chat_history[-1]
            if last_msg["user"]:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø§Ù„Ù…Ø±ÙŠØ¶):", value=last_msg['user'], height=100, disabled=True)
            else:
                st.text_area("Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© (Ø³Ø§Ù†Ø¯ÙŠ):", value=last_msg['assistant'], height=100, disabled=True)

        st.divider()
        st.subheader("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        if st.button("ğŸ—‘ï¸ Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
            st.session_state.chat_history = []
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
            greeting = assistant.generate_greeting()
            st.session_state.chat_history.append({"user": "", "assistant": greeting, "mode": "system"})
            st.rerun()
            
        st.info("""
        **Ù…Ù„Ø§Ø­Ø¸Ø§Øª:**
        - Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ³ØªØ®Ø¯Ù… Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø§Ù„Ù…ØªØµÙØ­ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª
        - Ù„Ù„ØªØ³Ø¬ÙŠÙ„ØŒ Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ø¨Ø¯Ø¡ ÙˆØªØ­Ø¯Ø« Ø¨ÙˆØ¶ÙˆØ­
        - Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ØŒ Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù
        - ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø­Ø³Ø¨ ØªÙØ¶ÙŠÙ„Ùƒ
        """)

if __name__ == "__main__":
    main()
