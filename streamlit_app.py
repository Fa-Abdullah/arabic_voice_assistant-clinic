"""
Arabic Voice Assistant - Streamlit Web Version
Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…Ù† Ø§Ù„Ù…ØªØµÙØ­ (streamlit-webrtc)
"""

import streamlit as st
import numpy as np
import wave
import tempfile
import os
import base64
import json
import time

from pathlib import Path
from vosk import Model, KaldiRecognizer
from gtts import gTTS
from openai import OpenAI
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode

# ---------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ----------------
st.set_page_config(
    page_title="ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†",
    page_icon="ğŸ¦·",
    layout="wide"
)

# ---------------- Ø§Ù„Ø­Ø§Ù„Ø© ----------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "assistant_ready" not in st.session_state:
    st.session_state.assistant_ready = False
if "first_message_sent" not in st.session_state:
    st.session_state.first_message_sent = False


# ---------------- Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ----------------
class ArabicVoiceAssistant:
    def __init__(self):
        self.openai_client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key="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"
        )
        self.model = None
        self.rec = None
        self.sample_rate = 16000

        self.system_prompt = """Ø£Ù†Øª Ø³Ø§Ù†Ø¯ÙŠØŒ Ù…ÙˆØ¸ÙØ© Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ÙÙŠ Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†.

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: Ø§Ù„Ø§Ø«Ù†ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ø¹Ø© Ù…Ù† 8 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 6 Ù…Ø³Ø§Ø¡Ù‹ØŒ Ø§Ù„Ø³Ø¨Øª Ù…Ù† 9 ØµØ¨Ø§Ø­Ø§Ù‹ Ø¥Ù„Ù‰ 3 Ù…Ø³Ø§Ø¡Ù‹
- Ø§Ù„Ù…ÙˆÙ‚Ø¹: ÙˆØ³Ø· Ù…Ø¯ÙŠÙ†Ø© ÙØ§Ù†ÙƒÙˆÙØ±
- Ø§Ù„Ù‡Ø§ØªÙ: (604) 555-DENTAL
- Ø§Ù„Ø®Ø¯Ù…Ø§Øª: Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¹Ø§Ù…ØŒ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø³Ù†Ø§Ù†ØŒ Ø§Ù„Ø­Ø´ÙˆØ§ØªØŒ Ø§Ù„ØªÙŠØ¬Ø§Ù†ØŒ Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ±ØŒ Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
1. Ø§Ø¬ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·
2. ÙƒÙˆÙ†ÙŠ ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©
3. Ø§Ø¬Ø¹Ù„ÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
4. Ø§Ø³Ø£Ù„ÙŠ Ø¯Ø§Ø¦Ù…Ø§Ù‹ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙƒØ«Ø±
5. Ø¹Ù†Ø¯ Ø­Ø¬Ø² Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ØŒ Ø§Ø·Ù„Ø¨ÙŠ Ø§Ø³Ù… Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
"""

    def find_and_load_model(self):
        try:
            search_paths = [Path.cwd(), Path.home() / "Downloads"]
            for search_dir in search_paths:
                if not search_dir.exists():
                    continue
                for item in search_dir.iterdir():
                    if (
                        item.is_dir()
                        and "vosk" in item.name.lower()
                        and "ar" in item.name.lower()
                    ):
                        if (item / "am").exists() and (item / "graph").exists():
                            self.model = Model(str(item))
                            self.rec = KaldiRecognizer(self.model, self.sample_rate)
                            return True, f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {item.name}"
            return False, "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Vosk Ø§Ù„Ø¹Ø±Ø¨ÙŠ"
        except Exception as e:
            return False, f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}"

    def transcribe_audio_file(self, audio_file_path):
        if not self.rec:
            return ""
        try:
            self.rec = KaldiRecognizer(self.model, self.sample_rate)
            with wave.open(audio_file_path, "rb") as wav_file:
                audio_data = wav_file.readframes(wav_file.getnframes())
                if self.rec.AcceptWaveform(audio_data):
                    result = json.loads(self.rec.Result())
                    text = result.get("text", "").strip()
                else:
                    result = json.loads(self.rec.FinalResult())
                    text = result.get("text", "").strip()
                return text
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ: {e}")
            return ""

    def generate_response(self, user_text):
        try:
            messages = [{"role": "system", "content": self.system_prompt}]
            for msg in st.session_state.chat_history[-5:]:
                messages.append({"role": "user", "content": msg["user"]})
                messages.append({"role": "assistant", "content": msg["assistant"]})
            messages.append({"role": "user", "content": user_text})

            response = self.openai_client.chat.completions.create(
                model="google/gemma-2-9b-it",
                messages=messages,
                max_tokens=200,
                temperature=0.7,
            )
            return response.choices[0].message.content.strip()
        except Exception:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    def text_to_speech(self, text):
        try:
            tts = gTTS(text=text, lang="ar", slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
            tts.save(temp_file.name)
            return temp_file.name
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª: {e}")
            return None


@st.cache_resource
def get_assistant():
    return ArabicVoiceAssistant()


# ---------------- Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ù…ØªØµÙØ­ ----------------
class AudioProcessor(AudioProcessorBase):
    def __init__(self) -> None:
        self.frames = []

    def recv_audio(self, frame):
        self.frames.append(frame.to_ndarray())
        return frame

    def save_wav(self, filename="temp.wav"):
        if not self.frames:
            return None
        audio = np.concatenate(self.frames, axis=0).astype(np.int16)
        with wave.open(filename, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(audio.tobytes())
        return filename


# ---------------- ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ----------------
st.title("ğŸ¦· Ø¹ÙŠØ§Ø¯Ø© ÙØ§Ù†ÙƒÙˆÙØ± Ù„Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†")
st.markdown("### ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ - Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒÙŠØ©")
st.info("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ù…Ø§ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø§Ù„Ù…ØªØµÙØ­ ğŸ¤")

assistant = get_assistant()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
if not st.session_state.assistant_ready:
    with st.spinner("ğŸ”„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Vosk..."):
        success, message = assistant.find_and_load_model()
        if success:
            st.success(message)
            st.session_state.assistant_ready = True
        else:
            st.error(message)
            st.stop()

# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ù…Ù† Ø§Ù„Ø¨Ø±Ø§ÙˆØ²Ø±
ctx = webrtc_streamer(
    key="speech",
    mode=WebRtcMode.SENDRECV,
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True,
)

if ctx and ctx.audio_processor:
    if st.button("ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„"):
        wav_file = ctx.audio_processor.save_wav()
        if wav_file:
            with st.spinner("ğŸ“ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ..."):
                text = assistant.transcribe_audio_file(wav_file)
            if text:
                st.success(f"âœ… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬: {text}")
                with st.spinner("ğŸ¤– ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯..."):
                    reply = assistant.generate_response(text)
                st.session_state.chat_history.append(
                    {"user": text, "assistant": reply}
                )
                st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {text}")
                st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {reply}")
                audio_file = assistant.text_to_speech(reply)
                if audio_file:
                    st.audio(audio_file, format="audio/mp3")
        else:
            st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø£ÙŠ ØµÙˆØª.")


# ---------------- Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ----------------
st.subheader("ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
for msg in st.session_state.chat_history:
    if msg["user"]:
        st.markdown(f"**ğŸ‘¤ Ø§Ù„Ù…Ø±ÙŠØ¶:** {msg['user']}")
    st.markdown(f"**ğŸ¤– Ø³Ø§Ù†Ø¯ÙŠ:** {msg['assistant']}")
